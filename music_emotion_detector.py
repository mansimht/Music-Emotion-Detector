# -*- coding: utf-8 -*-
"""Music Emotion Detector.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16E4OYf7Al8ggi4zk-vWrEU7Po07kPZht
"""

import os
import matplotlib.pyplot as plt
import seaborn as sns
import IPython.display as ipd
from IPython import display
import numpy as np
import librosa
import glob
import wave
import shutil
import sys
import csv
import gc
import time
from tqdm.notebook import tqdm
from PIL import Image
import pandas as pd


from scipy.fftpack import fft
from scipy import signal
from scipy.io import wavfile
from scipy.io import wavfile

import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout
from keras.layers.convolutional import Conv1D
from keras.layers.recurrent import LSTM
from keras.layers.convolutional import MaxPooling1D,AveragePooling1D
from keras.regularizers import l2
from tensorflow.keras import layers
from tensorflow.keras import models
from keras.utils import to_categorical

from sklearn.utils import shuffle
from sklearn.preprocessing import LabelEncoder
from keras.utils import np_utils
from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras.callbacks import TensorBoard

import random
random.seed(42)

"""# **Load .mp3 file and convert to .wav file**"""

songs= os.listdir('/content/memd-audio/MEMD_audio')

len(songs)

os.mkdir('./audio_files')

os.mkdir('./songs')

source_folder= '/content/memd-audio'
destination_folder= './audio_files'
#extension = input(.mp3)
for root,dirname,filenames in os.walk(source_folder):
  for filename in tqdm(filenames):
    if filename.endswith('{}'.format('.mp3')):
      shutil.copy(os.path.join(root,filename),destination_folder)

for root,dirname,filenames in os.walk('./audio_files'):
  audio_mp3= glob.glob(root+"/*.mp3")
  for a_file in tqdm(audio_mp3[0:600]):
    !spleeter separate '{a_file}' -o songs/

!zip -r /content/songs.zip /content/songs

from google.colab import files
files.download("/content/songs.zip")

"""# **Processing on Music files**"""

for root,dirnames,filenames in os.walk('/content/songs'):
  name= root.split('/')[-1]
  for files in tqdm(filenames):
    #vocal=files.split('.')[0]
    if files.startswith('accompaniment'):
      fname= root + '_' +files
      #print(fname)
      #print(os.path.join(root,files))
      shutil.copy(os.path.join(root,files),fname)

import re
music=[]
for root,dirnames,filenames in os.walk('/content/songs'):
  for files in tqdm(filenames):
    if  re.search('^\s*[0-9]',files):
      music.append(root + '/' +files)

def extract_feature(path):
    id = 1  # Song ID
    feature_set = pd.DataFrame()  # Feature Matrix
    
    # Individual Feature Vectors
    songname_vector = pd.Series()
    tempo_vector = pd.Series()
    total_beats = pd.Series()
    average_beats = pd.Series()
    chroma_stft_mean = pd.Series()
    chroma_stft_std = pd.Series()
    chroma_stft_var = pd.Series()
    chroma_cq_mean = pd.Series()
    chroma_cq_std = pd.Series()
    chroma_cq_var = pd.Series()
    chroma_cens_mean = pd.Series()
    chroma_cens_std = pd.Series()
    chroma_cens_var = pd.Series()
    mel_mean = pd.Series()
    mel_std = pd.Series()
    mel_var = pd.Series()
    mfcc_mean = pd.Series()
    mfcc_std = pd.Series()
    mfcc_var = pd.Series()
    mfcc_delta_mean = pd.Series()
    mfcc_delta_std = pd.Series()
    mfcc_delta_var = pd.Series()
    rmse_mean = pd.Series()
    rmse_std = pd.Series()
    rmse_var = pd.Series()
    cent_mean = pd.Series()
    cent_std = pd.Series()
    cent_var = pd.Series()
    spec_bw_mean = pd.Series()
    spec_bw_std = pd.Series()
    spec_bw_var = pd.Series()
    contrast_mean = pd.Series()
    contrast_std = pd.Series()
    contrast_var = pd.Series()
    rolloff_mean = pd.Series()
    rolloff_std = pd.Series()
    rolloff_var = pd.Series()
    poly_mean = pd.Series()
    poly_std = pd.Series()
    poly_var = pd.Series()
    tonnetz_mean = pd.Series()
    tonnetz_std = pd.Series()
    tonnetz_var = pd.Series()
    zcr_mean = pd.Series()
    zcr_std = pd.Series()
    zcr_var = pd.Series()
    harm_mean = pd.Series()
    harm_std = pd.Series()
    harm_var = pd.Series()
    perc_mean = pd.Series()
    perc_std = pd.Series()
    perc_var = pd.Series()
    frame_mean = pd.Series()
    frame_std = pd.Series()
    frame_var = pd.Series()
    
    
    # Traversing over each file in path
    file_data = path#[f for f in listdir(path) if isfile (join(path, f))]
    for line in tqdm(file_data):
        #if ( line[-1:] == '\n' ):
            #line = line[:-1]

        # Reading Song
        songname = line   
        name= songname.split('/')[6].split('_')[0]
        #print(name)
        # songname = audio.split('/')[5].split('.')[0]
        # s_id = re.findall(r'\d+', songname)    
        y, sr = librosa.load(songname, duration=60)
        S = np.abs(librosa.stft(y))
        
        # Extracting Features
        tempo, beats = librosa.beat.beat_track(y=y, sr=sr)
        chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)
        chroma_cq = librosa.feature.chroma_cqt(y=y, sr=sr)
        chroma_cens = librosa.feature.chroma_cens(y=y, sr=sr)
        melspectrogram = librosa.feature.melspectrogram(y=y, sr=sr)
        rmse = librosa.feature.rms(y=y)
        cent = librosa.feature.spectral_centroid(y=y, sr=sr)
        spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)
        contrast = librosa.feature.spectral_contrast(S=S, sr=sr)
        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
        poly_features = librosa.feature.poly_features(S=S, sr=sr)
        tonnetz = librosa.feature.tonnetz(y=y, sr=sr)
        zcr = librosa.feature.zero_crossing_rate(y)
        harmonic = librosa.effects.harmonic(y)
        percussive = librosa.effects.percussive(y)
        
        mfcc = librosa.feature.mfcc(y=y, sr=sr)
        mfcc_delta = librosa.feature.delta(mfcc)
    
        onset_frames = librosa.onset.onset_detect(y=y, sr=sr)
        frames_to_time = librosa.frames_to_time(onset_frames[:20], sr=sr)
        
        # Transforming Features 
        songname_vector.at[id]=name # song name
        tempo_vector.at[id]= tempo  # tempo
        total_beats.at[id]= sum(beats)  # beats
        average_beats.at[id] = np.average(beats)
        chroma_stft_mean.at[id]= np.mean(chroma_stft)  # chroma stft
        chroma_stft_std.at[id]= np.std(chroma_stft)
        chroma_stft_var.at[id]= np.var(chroma_stft)
        chroma_cq_mean.at[id]=np.mean(chroma_cq)  # chroma cq
        chroma_cq_std.at[id]= np.std(chroma_cq)
        chroma_cq_var.at[id]= np.var(chroma_cq)
        chroma_cens_mean.at[id]= np.mean(chroma_cens)  # chroma cens
        chroma_cens_std.at[id]= np.std(chroma_cens)
        chroma_cens_var.at[id]= np.var(chroma_cens)
        mel_mean.at[id]= np.mean(melspectrogram)  # melspectrogram
        mel_std.at[id]= np.std(melspectrogram)
        mel_var.at[id]= np.var(melspectrogram)
        mfcc_mean.at[id]= np.mean(mfcc)  # mfcc
        mfcc_std.at[id]= np.std(mfcc)
        mfcc_var.at[id]= np.var(mfcc)
        mfcc_delta_mean.at[id]= np.mean(mfcc_delta)  # mfcc delta
        mfcc_delta_std.at[id]= np.std(mfcc_delta)
        mfcc_delta_var.at[id]= np.var(mfcc_delta)
        rmse_mean.at[id]= np.mean(rmse) # rmse
        rmse_std.at[id]= np.std(rmse)
        rmse_var.at[id]=np.var(rmse)
        cent_mean.at[id]= np.mean(cent)  # cent
        cent_std.at[id]= np.std(cent)
        cent_var.at[id]= np.var(cent)
        spec_bw_mean.at[id]=np.mean(spec_bw)  # spectral bandwidth
        spec_bw_std.at[id]= np.std(spec_bw)
        spec_bw_var.at[id]= np.var(spec_bw)
        contrast_mean.at[id]= np.mean(contrast)  # contrast
        contrast_std.at[id]= np.std(contrast)
        contrast_var.at[id]= np.var(contrast)
        rolloff_mean.at[id]= np.mean(rolloff) # rolloff
        rolloff_std.at[id]= np.std(rolloff)
        rolloff_var.at[id]= np.var(rolloff)
        poly_mean.at[id]= np.mean(poly_features)  # poly features
        poly_std.at[id]= np.std(poly_features)
        poly_var.at[id]= np.var(poly_features)
        tonnetz_mean.at[id]= np.mean(tonnetz) # tonnetz
        tonnetz_std.at[id]= np.std(tonnetz)
        tonnetz_var.at[id]= np.var(tonnetz)
        zcr_mean.at[id]= np.mean(zcr)  # zero crossing rate
        zcr_std.at[id]= np.std(zcr)
        zcr_var.at[id]= np.var(zcr)
        harm_mean.at[id]= np.mean(harmonic)  # harmonic
        harm_std.at[id]= np.std(harmonic)
        harm_var.at[id]= np.var(harmonic)
        perc_mean.at[id]= np.mean(percussive)  # percussive
        perc_std.at[id]= np.std(percussive)
        perc_var.at[id]= np.var(percussive)
        frame_mean.at[id]= np.mean(frames_to_time)  # frames
        frame_std.at[id]=np.std(frames_to_time)
        frame_var.at[id]= np.var(frames_to_time)
        
        print(songname)
        print(name)
        id = id+1
    
    # Concatenating Features into one csv and json format
    feature_set['song_id'] = songname_vector  # song name
    feature_set['tempo'] = tempo_vector  # tempo 
    feature_set['total_beats'] = total_beats  # beats
    feature_set['average_beats'] = average_beats
    feature_set['chroma_stft_mean'] = chroma_stft_mean  # chroma stft
    feature_set['chroma_stft_std'] = chroma_stft_std
    feature_set['chroma_stft_var'] = chroma_stft_var
    feature_set['chroma_cq_mean'] = chroma_cq_mean  # chroma cq
    feature_set['chroma_cq_std'] = chroma_cq_std
    feature_set['chroma_cq_var'] = chroma_cq_var
    feature_set['chroma_cens_mean'] = chroma_cens_mean  # chroma cens
    feature_set['chroma_cens_std'] = chroma_cens_std
    feature_set['chroma_cens_var'] = chroma_cens_var
    feature_set['melspectrogram_mean'] = mel_mean  # melspectrogram
    feature_set['melspectrogram_std'] = mel_std
    feature_set['melspectrogram_var'] = mel_var
    feature_set['mfcc_mean'] = mfcc_mean  # mfcc
    feature_set['mfcc_std'] = mfcc_std
    feature_set['mfcc_var'] = mfcc_var
    feature_set['mfcc_delta_mean'] = mfcc_delta_mean  # mfcc delta
    feature_set['mfcc_delta_std'] = mfcc_delta_std
    feature_set['mfcc_delta_var'] = mfcc_delta_var
    feature_set['rmse_mean'] = rmse_mean  # rmse
    feature_set['rmse_std'] = rmse_std
    feature_set['rmse_var'] = rmse_var
    feature_set['cent_mean'] = cent_mean  # cent
    feature_set['cent_std'] = cent_std
    feature_set['cent_var'] = cent_var
    feature_set['spec_bw_mean'] = spec_bw_mean  # spectral bandwidth
    feature_set['spec_bw_std'] = spec_bw_std
    feature_set['spec_bw_var'] = spec_bw_var
    feature_set['contrast_mean'] = contrast_mean  # contrast
    feature_set['contrast_std'] = contrast_std
    feature_set['contrast_var'] = contrast_var
    feature_set['rolloff_mean'] = rolloff_mean  # rolloff
    feature_set['rolloff_std'] = rolloff_std
    feature_set['rolloff_var'] = rolloff_var
    feature_set['poly_mean'] = poly_mean  # poly features
    feature_set['poly_std'] = poly_std
    feature_set['poly_var'] = poly_var
    feature_set['tonnetz_mean'] = tonnetz_mean  # tonnetz
    feature_set['tonnetz_std'] = tonnetz_std
    feature_set['tonnetz_var'] = tonnetz_var
    feature_set['zcr_mean'] = zcr_mean  # zero crossing rate
    feature_set['zcr_std'] = zcr_std
    feature_set['zcr_var'] = zcr_var
    feature_set['harm_mean'] = harm_mean  # harmonic
    feature_set['harm_std'] = harm_std
    feature_set['harm_var'] = harm_var
    feature_set['perc_mean'] = perc_mean  # percussive
    feature_set['perc_std'] = perc_std
    feature_set['perc_var'] = perc_var
    feature_set['frame_mean'] = frame_mean  # frames
    feature_set['frame_std'] = frame_std
    feature_set['frame_var'] = frame_var
    
    # Converting Dataframe into CSV Excel and JSON file
    feature_set.to_csv('Emotion_features.csv')
    feature_set.to_json('Emotion_features.json')

extract_feature(music)

"""# **Processing of Labels**"""

arousal= pd.read_csv('/content/arousal.csv')
arousal.head(5)

valence= pd.read_csv('/content/valence.csv')
valence.head(5)

import re
ar_lst1=[]
val_lst1=[]
for root,dirnames,filenames in os.walk('/content/msicvocaldataset/content/songs'):
  name= root.split('/')[-1]
  for files in filenames:
    if files.startswith('accompaniment'):
      audio= root+files
      name= audio.split('/')[5].split('.')[0]
      s_id = re.findall(r'\d+', name)
      for id in s_id:
        id= int(id)
        ar_final= arousal.loc[arousal['song_id']==id]
        ar_lst1.append(ar_final)
        val_final= valence.loc[valence['song_id']==id]
        val_lst1.append(val_final)

arousal_final= pd.DataFrame()
arousal_final= pd.concat(ar_lst1)
print(arousal_final.shape)

arousal_new= arousal_final.mean(axis=1,skipna=True)
print(arousal_new.head(10))

valence_final= pd.DataFrame()
valence_final= pd.concat(val_lst1)
print(valence_final.shape)

valence_new= valence_final.mean(axis=1,skipna=True)
print(valence_new.head(10))

new_df= pd.DataFrame(columns=['song_id','arousal','valence'])
new_df['song_id']= valence_final.iloc[:,0]
new_df['arousal']= arousal_new
new_df['valence']= valence_new
new_df

"""Extract feeling from arosual and valence:

"Happy", # Valence > 0 and Arousal >0

"Calm", # Valence > 0 and Arousal < 0

"Angry", # Valence <0 and Arousal > 0

"Sad", # Valence < 0 and Arousal < 0
"""

def feeling_map(s):
    feeling=[]
    if s['valence'] >= 0 and s['arousal'] >= 0:
        feeling.append('Happy') 
    elif s['valence'] < 0 and s['arousal'] >= 0:
        feeling.append('Angry')
    elif s['valence'] < 0 and s['arousal'] < 0:
        feeling.append('Sad') 
    elif s['valence'] >= 0 and s['arousal'] < 0:
        feeling.append('Calm')
    return feeling

# Add a new column
new_df["Feeling"] = new_df.apply(feeling_map, axis=1)
new_df["Feeling"] = [f[0] for f in new_df['Feeling']]
# new_df["Feeling"]= new_df["Feeling"].astype('str')
print(new_df["Feeling"].unique())
new_df.dtypes

new_df= new_df.drop(['arousal','valence'],axis=1)
new_df

Emotion_features= pd.read_csv('/content/Emotion_features-final.csv')
Emotion_features1= Emotion_features.drop(['Unnamed: 0'],axis=1)
print(Emotion_features.head(10))

# Emotion_features=Emotion_features.drop_duplicates(['song_id'], keep=False)
# Emotion_features

combined_df= pd.merge(Emotion_features,new_df,on='song_id')
print(combined_df.shape)
combined_df=combined_df.drop_duplicates(['song_id'], keep=False)
print(combined_df.shape)
print(combined_df.head(10))

# # visualize the target variable

# g = sns.countplot(combined_df['Feeling'])
# g.set_xticklabels(['Happy','Sad','Calm'])
# plt.show()

# # class count
# class_count0,class_count1,class_count2= combined_df['Feeling'].value_counts()

# # Separate class
# class_0 = combined_df[combined_df['Feeling'] == 0]
# class_1 = combined_df[combined_df['Feeling'] == 1]
# class_2= combined_df[combined_df['Feeling'] == 2]
# print('class 0:', class_0.shape)
# print('class 1:', class_1.shape)
# print('class 2:', class_2.shape)

# class_1_over = class_1.sample(class_count0,replace=True)
# class_2_under = class_2.sample(class_count0,replace=True)

# df_test_over = pd.concat([class_0,class_1_over, class_1,class_2], axis=0)

# print("total class of 1 and0:",test_under['Class'].value_counts())# plot the count after under-sampeling
# test_under['Class'].value_counts().plot(kind='bar', title='count (target)')

"""# **Create Feature and Target Variable**"""

X = combined_df.iloc[:,1:55]
Y = combined_df.Feeling

# #For data imbalance
# from imblearn.under_sampling import RandomUnderSampler

# rus = RandomUnderSampler(return_indices=True)
# X_rus, y_rus, id_rus = rus.fit_sample(X, Y)

# print('Removed indexes:', id_rus)
# print(X_rus)
# print(y_rus)

X= combined_df.iloc[:,1:55]
y= combined_df.Feeling

le= LabelEncoder()
combined_df['Feeling']= le.fit_transform(combined_df['Feeling'])

X = combined_df.iloc[:,1:55]
Y = to_categorical(combined_df.Feeling, 3)

X_train,X_test,y_train,y_test= train_test_split(X,Y,test_size=0.2,random_state=5,shuffle=True)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

# NORMALIZE DATA
mean = np.mean(X_train, axis=0)
std = np.std(X_train, axis=0)
X_train = (X_train - mean)/std
X_test = (X_test - mean)/std

print(X_train.shape)
print(X_test.shape)

# TURN INTO ARRAYS
X_train = np.array(X_train)
y_train = np.array(y_train)
X_test = np.array(X_test)
y_test = np.array(y_test)

# # ONE HOT ENCODE EMOTION LABELS
# lb = LabelEncoder()
# y_train = to_categorical(lb.fit_transform(y_train))
# y_test = to_categorical(lb.fit_transform(y_test))

# RESHAPE TO ADD 3D TENSOR FOR 1D CNN INPUT
X_train = X_train[:,:,np.newaxis]
X_test = X_test[:,:,np.newaxis]
print(X_train.shape)
print(X_test.shape)

"""# **Model Creation**"""

import random
random.seed(42)

tf.keras.backend.clear_session()

def model(input_shape, num_classes):
    inputs = tf.keras.layers.Input(shape=input_shape, name="input")

    x = Conv1D(64, 9, activation='relu',padding='same')(inputs)
    x = Conv1D(64, 9, activation='relu',padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = MaxPooling1D(pool_size=2)(x)
    x= Dropout(0.4)(x)
    x = Conv1D(128,9, activation='relu',padding='same')(x)
    x = Conv1D(128,9, activation='relu',padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = MaxPooling1D(pool_size=2)(x)
    x= Dropout(0.4)(x)
    x = Conv1D(256,9, activation='relu',padding='same')(x)
    x = Conv1D(256,9, activation='relu',padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = MaxPooling1D(pool_size=2)(x)
    x= Dropout(0.4)(x)
    x= Flatten()(x)
    #x = Dense(128, activation="relu")(x)

    outputs = Dense(num_classes, activation="softmax", name="output")(x)

    return models.Model(inputs=inputs, outputs=outputs)

model = model((X_train.shape[1],1), 3)

model.summary()

# FIT MODEL
opt = Adam(lr=0.001)
model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])
model_history=model.fit(X_train,y_train,batch_size=64, epochs=5, validation_data=(X_test,y_test))

model.save("songs.h5")

"""# **Prediction of Song**"""

def extract(path):
    id = 1  # Song ID
    feature_set = pd.DataFrame()  # Feature Matrix
    
    # Individual Feature Vectors
    #songname_vector = pd.Series()
    tempo_vector = pd.Series()
    total_beats = pd.Series()
    average_beats = pd.Series()
    chroma_stft_mean = pd.Series()
    chroma_stft_std = pd.Series()
    chroma_stft_var = pd.Series()
    chroma_cq_mean = pd.Series()
    chroma_cq_std = pd.Series()
    chroma_cq_var = pd.Series()
    chroma_cens_mean = pd.Series()
    chroma_cens_std = pd.Series()
    chroma_cens_var = pd.Series()
    mel_mean = pd.Series()
    mel_std = pd.Series()
    mel_var = pd.Series()
    mfcc_mean = pd.Series()
    mfcc_std = pd.Series()
    mfcc_var = pd.Series()
    mfcc_delta_mean = pd.Series()
    mfcc_delta_std = pd.Series()
    mfcc_delta_var = pd.Series()
    rmse_mean = pd.Series()
    rmse_std = pd.Series()
    rmse_var = pd.Series()
    cent_mean = pd.Series()
    cent_std = pd.Series()
    cent_var = pd.Series()
    spec_bw_mean = pd.Series()
    spec_bw_std = pd.Series()
    spec_bw_var = pd.Series()
    contrast_mean = pd.Series()
    contrast_std = pd.Series()
    contrast_var = pd.Series()
    rolloff_mean = pd.Series()
    rolloff_std = pd.Series()
    rolloff_var = pd.Series()
    poly_mean = pd.Series()
    poly_std = pd.Series()
    poly_var = pd.Series()
    tonnetz_mean = pd.Series()
    tonnetz_std = pd.Series()
    tonnetz_var = pd.Series()
    zcr_mean = pd.Series()
    zcr_std = pd.Series()
    zcr_var = pd.Series()
    harm_mean = pd.Series()
    harm_std = pd.Series()
    harm_var = pd.Series()
    perc_mean = pd.Series()
    perc_std = pd.Series()
    perc_var = pd.Series()
    frame_mean = pd.Series()
    frame_std = pd.Series()
    frame_var = pd.Series()
    
    
    # Reading Song
    songname = path   
    #name= songname.split('/')[4].split('_')[0]
    #print(name)
    # songname = audio.split('/')[5].split('.')[0]
    # s_id = re.findall(r'\d+', songname)    
    y, sr = librosa.load(songname, duration=60)
    S = np.abs(librosa.stft(y))
    
    # Extracting Features
    tempo, beats = librosa.beat.beat_track(y=y, sr=sr)
    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)
    chroma_cq = librosa.feature.chroma_cqt(y=y, sr=sr)
    chroma_cens = librosa.feature.chroma_cens(y=y, sr=sr)
    melspectrogram = librosa.feature.melspectrogram(y=y, sr=sr)
    rmse = librosa.feature.rms(y=y)
    cent = librosa.feature.spectral_centroid(y=y, sr=sr)
    spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)
    contrast = librosa.feature.spectral_contrast(S=S, sr=sr)
    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
    poly_features = librosa.feature.poly_features(S=S, sr=sr)
    tonnetz = librosa.feature.tonnetz(y=y, sr=sr)
    zcr = librosa.feature.zero_crossing_rate(y)
    harmonic = librosa.effects.harmonic(y)
    percussive = librosa.effects.percussive(y)
    
    mfcc = librosa.feature.mfcc(y=y, sr=sr)
    mfcc_delta = librosa.feature.delta(mfcc)

    onset_frames = librosa.onset.onset_detect(y=y, sr=sr)
    frames_to_time = librosa.frames_to_time(onset_frames[:20], sr=sr)
    
    # Transforming Features 
    #songname_vector.at[id]=name # song name
    tempo_vector.at[id]= tempo  # tempo
    total_beats.at[id]= sum(beats)  # beats
    average_beats.at[id] = np.average(beats)
    chroma_stft_mean.at[id]= np.mean(chroma_stft)  # chroma stft
    chroma_stft_std.at[id]= np.std(chroma_stft)
    chroma_stft_var.at[id]= np.var(chroma_stft)
    chroma_cq_mean.at[id]=np.mean(chroma_cq)  # chroma cq
    chroma_cq_std.at[id]= np.std(chroma_cq)
    chroma_cq_var.at[id]= np.var(chroma_cq)
    chroma_cens_mean.at[id]= np.mean(chroma_cens)  # chroma cens
    chroma_cens_std.at[id]= np.std(chroma_cens)
    chroma_cens_var.at[id]= np.var(chroma_cens)
    mel_mean.at[id]= np.mean(melspectrogram)  # melspectrogram
    mel_std.at[id]= np.std(melspectrogram)
    mel_var.at[id]= np.var(melspectrogram)
    mfcc_mean.at[id]= np.mean(mfcc)  # mfcc
    mfcc_std.at[id]= np.std(mfcc)
    mfcc_var.at[id]= np.var(mfcc)
    mfcc_delta_mean.at[id]= np.mean(mfcc_delta)  # mfcc delta
    mfcc_delta_std.at[id]= np.std(mfcc_delta)
    mfcc_delta_var.at[id]= np.var(mfcc_delta)
    rmse_mean.at[id]= np.mean(rmse) # rmse
    rmse_std.at[id]= np.std(rmse)
    rmse_var.at[id]=np.var(rmse)
    cent_mean.at[id]= np.mean(cent)  # cent
    cent_std.at[id]= np.std(cent)
    cent_var.at[id]= np.var(cent)
    spec_bw_mean.at[id]=np.mean(spec_bw)  # spectral bandwidth
    spec_bw_std.at[id]= np.std(spec_bw)
    spec_bw_var.at[id]= np.var(spec_bw)
    contrast_mean.at[id]= np.mean(contrast)  # contrast
    contrast_std.at[id]= np.std(contrast)
    contrast_var.at[id]= np.var(contrast)
    rolloff_mean.at[id]= np.mean(rolloff) # rolloff
    rolloff_std.at[id]= np.std(rolloff)
    rolloff_var.at[id]= np.var(rolloff)
    poly_mean.at[id]= np.mean(poly_features)  # poly features
    poly_std.at[id]= np.std(poly_features)
    poly_var.at[id]= np.var(poly_features)
    tonnetz_mean.at[id]= np.mean(tonnetz) # tonnetz
    tonnetz_std.at[id]= np.std(tonnetz)
    tonnetz_var.at[id]= np.var(tonnetz)
    zcr_mean.at[id]= np.mean(zcr)  # zero crossing rate
    zcr_std.at[id]= np.std(zcr)
    zcr_var.at[id]= np.var(zcr)
    harm_mean.at[id]= np.mean(harmonic)  # harmonic
    harm_std.at[id]= np.std(harmonic)
    harm_var.at[id]= np.var(harmonic)
    perc_mean.at[id]= np.mean(percussive)  # percussive
    perc_std.at[id]= np.std(percussive)
    perc_var.at[id]= np.var(percussive)
    frame_mean.at[id]= np.mean(frames_to_time)  # frames
    frame_std.at[id]=np.std(frames_to_time)
    frame_var.at[id]= np.var(frames_to_time)
    
    print(songname)
    #print(name)
    
    
    # Concatenating Features into one csv and json format
    #feature_set['song_id'] = songname_vector  # song name
    feature_set['tempo'] = tempo_vector  # tempo 
    feature_set['total_beats'] = total_beats  # beats
    feature_set['average_beats'] = average_beats
    feature_set['chroma_stft_mean'] = chroma_stft_mean  # chroma stft
    feature_set['chroma_stft_std'] = chroma_stft_std
    feature_set['chroma_stft_var'] = chroma_stft_var
    feature_set['chroma_cq_mean'] = chroma_cq_mean  # chroma cq
    feature_set['chroma_cq_std'] = chroma_cq_std
    feature_set['chroma_cq_var'] = chroma_cq_var
    feature_set['chroma_cens_mean'] = chroma_cens_mean  # chroma cens
    feature_set['chroma_cens_std'] = chroma_cens_std
    feature_set['chroma_cens_var'] = chroma_cens_var
    feature_set['melspectrogram_mean'] = mel_mean  # melspectrogram
    feature_set['melspectrogram_std'] = mel_std
    feature_set['melspectrogram_var'] = mel_var
    feature_set['mfcc_mean'] = mfcc_mean  # mfcc
    feature_set['mfcc_std'] = mfcc_std
    feature_set['mfcc_var'] = mfcc_var
    feature_set['mfcc_delta_mean'] = mfcc_delta_mean  # mfcc delta
    feature_set['mfcc_delta_std'] = mfcc_delta_std
    feature_set['mfcc_delta_var'] = mfcc_delta_var
    feature_set['rmse_mean'] = rmse_mean  # rmse
    feature_set['rmse_std'] = rmse_std
    feature_set['rmse_var'] = rmse_var
    feature_set['cent_mean'] = cent_mean  # cent
    feature_set['cent_std'] = cent_std
    feature_set['cent_var'] = cent_var
    feature_set['spec_bw_mean'] = spec_bw_mean  # spectral bandwidth
    feature_set['spec_bw_std'] = spec_bw_std
    feature_set['spec_bw_var'] = spec_bw_var
    feature_set['contrast_mean'] = contrast_mean  # contrast
    feature_set['contrast_std'] = contrast_std
    feature_set['contrast_var'] = contrast_var
    feature_set['rolloff_mean'] = rolloff_mean  # rolloff
    feature_set['rolloff_std'] = rolloff_std
    feature_set['rolloff_var'] = rolloff_var
    feature_set['poly_mean'] = poly_mean  # poly features
    feature_set['poly_std'] = poly_std
    feature_set['poly_var'] = poly_var
    feature_set['tonnetz_mean'] = tonnetz_mean  # tonnetz
    feature_set['tonnetz_std'] = tonnetz_std
    feature_set['tonnetz_var'] = tonnetz_var
    feature_set['zcr_mean'] = zcr_mean  # zero crossing rate
    feature_set['zcr_std'] = zcr_std
    feature_set['zcr_var'] = zcr_var
    feature_set['harm_mean'] = harm_mean  # harmonic
    feature_set['harm_std'] = harm_std
    feature_set['harm_var'] = harm_var
    feature_set['perc_mean'] = perc_mean  # percussive
    feature_set['perc_std'] = perc_std
    feature_set['perc_var'] = perc_var
    feature_set['frame_mean'] = frame_mean  # frames
    feature_set['frame_std'] = frame_std
    feature_set['frame_var'] = frame_var
    
    return feature_set
    # # Converting Dataframe into CSV Excel and JSON file
    # feature_set.to_csv('Emotion_features.csv')
    # feature_set.to_json('Emotion_features.json')

!spleeter separate '/content/Passenger let her go.mp3' -o output/

path='/content/output/Passenger let her go/accompaniment.wav'

song= extract(path)
song

song

le.classes_

def prediction_result(song):
  ans=model.predict(song)
  final= np.argmax(ans, axis=1)
  labels = {0:'Calm',1:'Happy', 2:'Sad'}
  print(f"Predicted Result : {labels[final[0]]}")

prediction_result(song1)
